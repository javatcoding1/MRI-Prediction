{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javatcoding1/MRI-Prediction/blob/main/site/en/gemini-api/docs/vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084u8u0DpBlo"
      },
      "source": [
        "# Explore vision capabilities with the Gemini API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWzQEqNosrS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs/vision\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/vision.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/vision.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c5e92a74e64"
      },
      "source": [
        "The Gemini API is able to process images and videos, enabling a multitude of\n",
        " exciting developer use cases. Some of Gemini's vision capabilities include\n",
        " the ability to:\n",
        "\n",
        "*   Caption and answer questions about images\n",
        "*   Transcribe and reason over PDFs, including long documents up to 2 million token context window\n",
        "*   Describe, segment, and extract information from videos,\n",
        "including both visual frames and audio, up to 90 minutes long\n",
        "*   Detect objects in an image and return bounding box coordinates for them\n",
        "\n",
        "This tutorial demonstrates some possible ways to prompt the Gemini API with\n",
        "images and video input, provides code examples,\n",
        "and outlines prompting best practices with multimodal vision capabilities.\n",
        "All output is text-only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxCstRHvpX0r"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before you use the File API, you need to install the Gemini API SDK package and configure an API key. This section describes how to complete these setup steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6J_rV2ipmj_"
      },
      "source": [
        "### Install the Python SDK and import packages\n",
        "\n",
        "The Python SDK for the Gemini API is contained in the [google-generativeai](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mN8x8DPgu9Kv"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NInUZ4hwDq6d"
      },
      "source": [
        "Import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0x3xmmWrDtEH"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8g4hTRotheH"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "The File API uses API keys for authentication and access. Uploaded files are associated with the project linked to the API key. Unlike other Gemini APIs that use API keys, your API key also grants access to data you've uploaded to the File API, so take extra care in keeping your API key secure. For more on keeping your keys\n",
        "secure, see [Best practices for using API\n",
        "keys](https://support.google.com/googleapi/answer/6310037).\n",
        "\n",
        "Store your API key in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or are unfamiliar with Colab Secrets, refer to the [Authentication quickstart](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d6lYXRcjthKV"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XCDVCl3Gtodo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-z4zsCUlaru"
      },
      "source": [
        "## Prompting with images\n",
        "\n",
        "In this tutorial, you will upload images using the File API or as inline data and generate content based on those images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNehP2tr3Cr"
      },
      "source": [
        "### Technical details (images)\n",
        "Gemini 1.5 Pro and Flash support a maximum of 3,600 image files.\n",
        "\n",
        "Images must be in one of the following image data [MIME types](https://developers.google.com/drive/api/guides/ref-export-formats):\n",
        "\n",
        "-   PNG - `image/png`\n",
        "-   JPEG - `image/jpeg`\n",
        "-   WEBP - `image/webp`\n",
        "-   HEIC - `image/heic`\n",
        "-   HEIF - `image/heif`\n",
        "\n",
        "Each image is equivalent to 258 tokens.\n",
        "\n",
        "While there are no specific limits to the number of pixels in an image besides the model’s context window, larger images are scaled down to a maximum resolution of 3072x3072 while preserving their original aspect ratio, while smaller images are scaled up to 768x768 pixels. There is no cost reduction for images at lower sizes, other than bandwidth, or performance improvement for images at higher resolution.\n",
        "\n",
        "For best results:\n",
        "\n",
        "*   Rotate images to the correct orientation before uploading.\n",
        "*   Avoid blurry images.\n",
        "*   If using a single image, place the text prompt after the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fa34d5c0db8"
      },
      "source": [
        "## Image input\n",
        "\n",
        "For total image payload size less than 20MB, it's recommended to either upload\n",
        "base64 encoded images or directly upload locally stored image files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8336e412da3e"
      },
      "source": [
        "### Base64 encoded images\n",
        "\n",
        "You can upload public image URLs by encoding them as Base64 payloads.\n",
        "You can use the httpx library to fetch the image URLs.\n",
        "The following code example shows how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aa9a0e452544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "029eaf71-9527-4894-ee0c-830438d47ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Scan Type:** Microscopic image (likely H&E stain)\n",
            "- **Organ:**  Placenta\n",
            "- **Tumor Type:** Not evident\n",
            "- **Tumor Subclass:** Not applicable\n",
            "- **Detailed Description:**  Image shows placental villi and intervillous space. No obvious abnormalities are readily apparent. \n",
            "- **Possible Causes:** Not applicable (no pathology identified)\n",
            "- **Clinical Insights:** Normal placental tissue. Further clinical correlation and potentially additional sections would be needed for a full diagnosis.  The white spaces suggest tissue separation artifact, a common occurrence in slide preparation.\n",
            "\n",
            "Scan Type: - **** Microscopic image (likely H&E stain)\n",
            "Organ Type: - ****  Placenta\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "import base64\n",
        "import mimetypes\n",
        "\n",
        "# Path to the image\n",
        "image_path = \"/content/SOB_M_DC-14-2523-400-005.png\"  # Replace with any image\n",
        "\n",
        "# Detect MIME type dynamically\n",
        "mime_type = mimetypes.guess_type(image_path)[0] or \"image/jpeg\"  # Default to JPEG if unknown\n",
        "\n",
        "# Read image file as binary data\n",
        "with open(image_path, \"rb\") as image_file:\n",
        "    image_data = image_file.read()\n",
        "\n",
        "# Choose a Gemini model\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
        "\n",
        "# System prompt for structured output\n",
        "prompt = \"\"\"You are analyzing a medical scan image. Provide the following structured output:\n",
        "\n",
        "- **Scan Type:** (e.g., MRI, CT Scan, X-ray)\n",
        "- **Organ:** (e.g., Brain, Lung, Heart,Breast)\n",
        "- **Tumor Type:** (If detected, specify type)\n",
        "- **Tumor Subclass:** (If applicable)\n",
        "- **Detailed Description:** (Size, shape, and location if possible)\n",
        "- **Possible Causes:** (List genetic factors, environmental exposure, lifestyle)\n",
        "- **Clinical Insights:** (Any relevant medical observations)\n",
        "\n",
        "Ensure the output is structured in plain text without any extra information or unrelated content.\"\"\"\n",
        "\n",
        "# Generate response\n",
        "response = model.generate_content(\n",
        "    [\n",
        "        {\n",
        "            \"mime_type\": mime_type,\n",
        "            \"data\": base64.b64encode(image_data).decode(\"utf-8\"),\n",
        "        },\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract response text\n",
        "response_text = response.text\n",
        "\n",
        "# Print the structured response\n",
        "print(response_text)\n",
        "\n",
        "# Extract specific values (optional)\n",
        "lines = response_text.split(\"\\n\")\n",
        "scan_type = lines[0].replace(\"Scan Type:\", \"\").strip() if len(lines) > 0 else \"Unknown\"\n",
        "organ_type = lines[1].replace(\"Organ:\", \"\").strip() if len(lines) > 1 else \"Unknown\"\n",
        "\n",
        "# Print extracted values (for further use)\n",
        "print(\"Scan Type:\", scan_type)\n",
        "print(\"Organ Type:\", organ_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47333dabe62"
      },
      "source": [
        "### Multiple images\n",
        "\n",
        "To prompt with multiple images in Base64 encoded format, you can do the\n",
        "following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2816ea3d2d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "570a2fd6-8f0e-42d4-87d9-133e17dbeea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- **** Lung'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "organ_type"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if organ_type == \"- **** Brain\":\n",
        "  import tensorflow as tf\n",
        "  import numpy as np\n",
        "  from tensorflow.keras.models import load_model\n",
        "  from tensorflow.keras.preprocessing import image\n",
        "\n",
        "  # Load the trained model\n",
        "  model = load_model(\"brain_model.h5\")\n",
        "\n",
        "  # Model summary (optional, to verify)\n",
        "  model.summary()\n",
        "\n",
        "  # Function to preprocess an input image\n",
        "  def preprocess_image(img_path):\n",
        "      img_size = (299, 299)  # Match model's input shape\n",
        "      img = image.load_img(img_path, target_size=img_size)\n",
        "      img_array = image.img_to_array(img)  # Convert to numpy array\n",
        "      img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "      img_array = tf.keras.applications.xception.preprocess_input(img_array)  # Apply Xception preprocessing\n",
        "      return img_array\n",
        "\n",
        "  # Load and preprocess an MRI image\n",
        "  img_path = \"/content/men.jpeg\"  # Replace with your image path\n",
        "  processed_img = preprocess_image(img_path)\n",
        "\n",
        "  # Perform prediction\n",
        "  predictions = model.predict(processed_img)\n",
        "\n",
        "  # Decode predictions (assuming 4 classes)\n",
        "  class_labels = [\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]  # Replace with actual class names\n",
        "  predicted_class = class_labels[np.argmax(predictions)]\n",
        "\n",
        "  # Print results\n",
        "  print(\"Predicted Class:\", predicted_class)\n",
        "  print(\"Prediction Probabilities:\", predictions)\n",
        "else:\n",
        "  import tensorflow as tf\n",
        "  from tensorflow.keras.models import load_model\n",
        "  from tensorflow.keras.preprocessing import image\n",
        "  import numpy as np\n",
        "\n",
        "  # Load the trained model\n",
        "  model = load_model(\"/content/breast_tumor.h5\")  # Ensure the correct file path\n",
        "\n",
        "  # Function to preprocess input image\n",
        "  def preprocess_image(img_path):\n",
        "      img = image.load_img(img_path, target_size=(244, 244))  # Resize to match model input\n",
        "      img_array = image.img_to_array(img)\n",
        "      img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "      img_array = img_array / 255.0  # Normalize pixel values\n",
        "      return img_array\n",
        "\n",
        "  # Path to the test image\n",
        "  img_path = \"/content/SOB_M_DC-14-2523-400-005.png\"  # Change this to your actual image path\n",
        "\n",
        "  # Preprocess and predict\n",
        "  img_array = preprocess_image(img_path)\n",
        "  prediction = model.predict(img_array)\n",
        "\n",
        "  # Interpret the results\n",
        "  class_labels = [\"Benign\", \"Malignant\"]  # Adjust labels if different\n",
        "  predicted_class = np.argmax(prediction)  # Get class index\n",
        "\n",
        "  print(f\"Predicted Class: {class_labels[predicted_class]}\")\n",
        "  print(f\"Confidence Scores: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH7gFC7jmaKU",
        "outputId": "da52fe8d-9660-4c34-c0aa-448059319b6d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "Predicted Class: Malignant\n",
            "Confidence Scores: [[0.19755012 0.8024499 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm862F3zthiI"
      },
      "source": [
        "### Upload one or more locally stored image files\n",
        "\n",
        "Alternatively, you can upload one or more locally stored image files..\n",
        "\n",
        "You can download and use our drawings of [piranha-infested waters](https://storage.googleapis.com/generativeai-downloads/images/piranha.jpg) and a [firefighter with a cat](https://storage.googleapis.com/generativeai-downloads/images/firefighter.jpg). First, save these files to your local directory.\n",
        "\n",
        "Then click **Files** on the left sidebar. For each file, click the **Upload** button, then navigate to that file's location and upload it:\n",
        "\n",
        "<img width=400 src=\"https://ai.google.dev/tutorials/images/colab_upload.png\">\n",
        "\n",
        "When the combination of files and system instructions that you intend to send is larger than 20 MB in size, use the File API to upload those files. Smaller files can instead be called locally from the Gemini API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzMhQ8MWub5_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "vision.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}